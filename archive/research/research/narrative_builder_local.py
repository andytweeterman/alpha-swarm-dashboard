import requests
import json
import os
import sys

# ==========================================
# CONFIGURATION
# ==========================================
# This is the default port for Ollama. 
# Ensure Ollama is running in the background!
OLLAMA_URL = "http://localhost:11434/api/generate"

# The model to use. 'llama3.1' is the best balance of speed/smarts for CPU.
# He must run "ollama pull llama3.1" in PowerShell first.
MODEL_NAME = "llama3.1"  

# ==========================================
# 1. LOAD THE PROMPT
# ==========================================
# We look for the "Quant Prompt" file generated by narrative_builder_v2.py
script_dir = os.path.dirname(os.path.abspath(__file__))
prompt_file = os.path.join(script_dir, "FINAL_QUANT_PROMPT.txt")

if not os.path.exists(prompt_file):
    print("\n[ERROR] Prompt file not found!")
    print(f"Looked for: {prompt_file}")
    print("Please run 'narrative_builder_v2.py' first to generate the prompt.")
    input("\nPress ENTER to exit...")
    sys.exit()

with open(prompt_file, "r") as f:
    full_prompt = f.read()

print(f"--- [ALPHA SWARM: LOCAL INFERENCE ENGINE] ---")
print(f"Target: {OLLAMA_URL}")
print(f"Model:  {MODEL_NAME}")
print(f"Prompt Loaded: {len(full_prompt)} characters")

# ==========================================
# 2. CALL THE LOCAL LLM (Ollama)
# ==========================================
print("\nStatus: Thinking... (This runs on CPU, give it 30-60s)")

payload = {
    "model": MODEL_NAME,
    "prompt": full_prompt,
    "stream": False,       # False = Wait for the whole response (simpler)
    "temperature": 0.2     # Low temp = More scientific/consistent output
}

try:
    # Send the request to the local machine
    response = requests.post(OLLAMA_URL, json=payload)
    response.raise_for_status()
    
    # Parse the JSON response
    result_json = response.json()
    final_text = result_json.get("response", "")
    
    # ==========================================
    # 3. OUTPUT & SAVE
    # ==========================================
    print("\n" + "="*50)
    print("   GENERATED BRIEFING (LOCAL)")
    print("="*50)
    print(final_text)
    print("="*50)
    
    # Save Output to text file
    output_path = os.path.join(script_dir, "Morning_Briefing_Local.txt")
    with open(output_path, "w") as f:
        f.write(final_text)
    print(f"\n[SUCCESS] Briefing saved to: {output_path}")

except requests.exceptions.ConnectionError:
    print("\n[CRITICAL ERROR] Could not connect to Ollama.")
    print("---------------------------------------------------")
    print("1. Is Ollama installed? (Download from ollama.com)")
    print("2. Is it running? (You should see the llama icon in the taskbar)")
    print("3. Did you pull the model? (Run 'ollama pull llama3.1' in terminal)")
    print("---------------------------------------------------")

except Exception as e:
    print(f"\n[ERROR] Something went wrong: {e}")

input("\nPress ENTER to exit...")